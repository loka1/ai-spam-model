# -*- coding: utf-8 -*-
"""spam_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/196QS0j5VsceTmziFgvvMLs2V_nYcj-cx
"""

!pip install -r tensorflow-spam/requirements.txt

"""# Importing Packages

Import packages
"""

import pandas as pd
import numpy as np
import tqdm
import nltk
import time
import random
import seaborn as sns
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from operator import itemgetter
from PIL import Image

# Commented out IPython magic to ensure Python compatibility.
import keras as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, recall_score, precision_score
from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.metrics import Recall, Precision
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix

# %load_ext tensorboard

"""# Load Data and Preprocess

Load the messages
"""

df = pd.read_csv('tensorflow-spam/messages.csv')
df.head()
df_orig = df.copy()

df_orig.shape

pd.set_option('display.max_colwidth', None)

print(df_orig.head(5))

df_orig.drop_duplicates(subset=['Message'], keep='first', inplace=True)

df_orig.shape

df_orig.value_counts('Category', normalize=True)

"""Create a binary target where 1 = Spam and 0 = Ham. Rename the columns."""

df['target'] = [1 if x=='spam' else 0 for  x in df.Category]
df.drop('Category', axis=1, inplace=True)
df.columns = ['message','target']
df.drop_duplicates(subset=['message'], keep='first', inplace=True)
df_spam = df[df.target==1]
df_ham = df[df.target==0]

"""Observe the baseline accuracy"""

df.target.value_counts(normalize=True)

"""Baseline is 86.6%

Define X and y datasets, or predictors and target datasets.
"""

X= df.message
y= df.target

"""I have left punctuation and non-alphanumeric characters in here for now because they are harmless. The Tensorflow Tokenizer can deal with them.

# EDA Plots and Word Clouds

## EDA Plots
"""

df_orig['word_count'] = df_orig.Message.apply(lambda x: len(x.split(' ')))

df_orig.groupby('Category')['word_count'].agg(['count','mean','median'])

df_orig.sample(5)

# Plot message length by spam/ham

df_orig['Category'] = pd.Categorical(df_orig['Category'], ['spam','ham'])
sns.set_style("whitegrid")

fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16,5))
sns.histplot(data=df_orig,
             x="word_count",
             hue="Category",
             palette='husl',
             bins=75,
             ax=ax[0],
             legend=True).set_title('Distribution of message length by Spam/Ham')
ax[0].set_xlabel('Message word count')
ax[0].set_ylabel('Count')
colors = sns.color_palette('husl')

ax[1].pie(df_orig.Category.value_counts(normalize=True), labels = ['Ham','Spam'], autopct='%0.f%%')
ax[1].set_title('Dataset message composition by Spam/Ham')
plt.savefig('tensorflow-spam/images/eda.png',bbox_inches='tight',dpi=400, pad_inches=0.1)
plt.show()

df_orig.sort_values(by='word_count', ascending=False)

"""## Word Clouds"""

# count word appearances
def word_counter(mat):
    count = np.array(mat.sum(axis=0))[0]
    return count

# sort words by frequency
def word_frequency_sorter(words, counts):
    sort_result = sorted(zip(words, list(counts)),
                         key=itemgetter(1), reverse=True)
    return sort_result

nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')

# Series text column
tvec = TfidfVectorizer(stop_words = stop,
                       ngram_range = (1, 2),
                       max_features = 1000,
                       token_pattern = '[A-Za-z0-9]+(?=\\s+)',
                        )

# Fit vectorizer on text data
tvec.fit(df.message)

# Transform data
tvec_mat = tvec.transform(df.message)

# words occuring
words = tvec.get_feature_names_out()

# count word appearances
csum= word_counter(tvec_mat)

# sort words by frequency
df_word_frequency = pd.DataFrame(word_frequency_sorter(
    words, csum), columns=["word", "frequency_series"])
df_word_frequency.set_index('word', inplace=True)
df_word_frequency.index.name = None
df_word_frequency.head(20)

# all description text
wrds_cnt = " ".join(wrds for wrds in df.message)
print ("There are {} unique words in all the descriptions.".format(len(wrds_cnt)))

# create stopword list:
stopwords = set(STOPWORDS)

# create word cloud
wc_all = WordCloud(stopwords=stopwords,
                                  max_words=200,
                                  background_color="white")

# generate a word cloud image
wc_all.generate(wrds_cnt)

# ham description text
wrds_cnt = " ".join(wrds for wrds in df_ham.message)
print ("There are {} unique words in ham the descriptions.".format(len(wrds_cnt)))

# create word mask
mask_ham = np.array(Image.open('tensorflow-spam/images/ham2.png'))

# create word cloud
wc_ham = WordCloud(stopwords=STOPWORDS,
               mask=mask_ham, background_color="white",
               max_words=2000, max_font_size=256,
               random_state=42, width=mask_ham.shape[1],
               height=mask_ham.shape[0])
wc_ham.generate(wrds_cnt);

# spam description text
wrds_cnt = " ".join(wrds for wrds in df_spam.message)
print ("There are {} unique words in spam the descriptions.".format(len(wrds_cnt)))

# create word mask
mask_spam = np.array(Image.open('tensorflow-spam/images/spam2.png'))

# create word cloud
wc_spam = WordCloud(stopwords=STOPWORDS,
               mask=mask_spam, background_color="white",
               max_words=2000, max_font_size=256,
               random_state=42, width=mask_spam.shape[1],
               height=mask_spam.shape[0])
wc_spam.generate(wrds_cnt);

# combine wordcloud plots
sns.set_style("white")
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,8))
ax[0].imshow(wc_spam, interpolation="bilinear")
ax[0].set_title('Spam word cloud')
ax[1].imshow(wc_ham, interpolation="bilinear")
ax[1].set_title('Ham word cloud')
fig.set_facecolor("white")
ax[0].axis('off')
ax[1].axis('off')
plt.savefig('tensorflow-spam/images/cloud.png',bbox_inches='tight',dpi=400, pad_inches=0.1)
plt.show()

"""# Tensorflow Modelling"""

# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/

SEQUENCE_LENGTH = 50 # the length of all sequences (number of words per sample)
EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors
TEST_SIZE = 0.25 # ratio of testing set

BATCH_SIZE = 64
EPOCHS = 20 # number of epochs

"""## Tokenize"""

# Text tokenization - we have to do this before obtaining the word embeddings
# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/

# vectorizing text, turning each text into sequence of integers.
# Default behaviour is to filter all punctuation, plus tabs and line breaks, minus the ' character.
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
# convert to sequence of integers
X = tokenizer.texts_to_sequences(X)

# Save this dictionary for later
word_index = tokenizer.word_index

print(X[1])

# pad sequences at the beginning of each sequence with 0's
# for example if SEQUENCE_LENGTH=4:
# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]
# will be transformed to:
# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]
X = pad_sequences(sequences=X,
                  maxlen=SEQUENCE_LENGTH,
                  padding='pre',
                  truncating='post')

print(X.shape)

"""Discuss what this 100 term is"""

X

y

"""## Train-Test Split"""

# split and shuffle
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=5)
# print our data shapes
print("X_train.shape:", X_train.shape)
print("X_test.shape:", X_test.shape)
print("y_train.shape:", y_train.shape)
print("y_test.shape:", y_test.shape)

"""## Word Embedding Using GloVe

I'm using a pre-trained [GloVe word embedding](https://nlp.stanford.edu/projects/glove/) dataset in 100 dimensional space, saved outside of this repository.

This function opens the GloVe file and maps each of the tokenized words to the corresponding embedded vector. This process works to encode meanings of words in a way that causes words of comparable meaning to be expressed similarly in the defined vector space. More can be read about word embedding [here](https://machinelearningmastery.com/what-are-word-embeddings/).
"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

# https://machinelearningmastery.com/what-are-word-embeddings/
# https://nlp.stanford.edu/projects/glove/

def get_embedding_vectors(tokenizer, dim=100):
    embedding_index = {}
    with open(f"glove.6B.{dim}d.txt", encoding='utf8') as f:
        for line in tqdm.tqdm(f, "Reading GloVe"):
            values = line.split()
            word = values[0]
            vectors = np.asarray(values[1:], dtype='float32')
            embedding_index[word] = vectors

    word_index = tokenizer.word_index
    embedding_matrix = np.zeros((len(word_index)+1, dim))
    for word, i in word_index.items():
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            # words not found will be 0s
            embedding_matrix[i] = embedding_vector

    return embedding_matrix

"""## BiLSTM Model Definition

Define a list of metrics to be used later in the model definition. These will be calculated and saved after each epoch in the model build for visibility on the modelling process.
"""

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR')] # precision-recall curve

"""Now to define the Keras model. I'm going to do this using the [sequential model API](https://machinelearningmastery.com/keras-functional-api-deep-learning/) where Sequential class is created and model layers are then added to it. The steps I use are as follows:
* Embed the tokenized words
* Add a user defined number of Bi-directional Long Short-Term Memory units
* Add a Dropout feature of 0.2 to combat over-fitting
* Output one dense layer with a sigmoid activation function for 1/0 classification
* Compile the model using [binary crossentropy](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/) as the loss function and rmsprop optimizer, reporting the extensive list of metrics after each epoch

With training the model through back-propagation, RNNs famously suffer from the vanishing gradient problem. This means that the RNN would struggle to learn long-range dependencies from the early layers and is often described as a short-term memory problem. To combat this we used specialised units within the hidden layers - Long Short-Term Memory units or LSTMs. These LSTMs are able to learn long-range dependencies through a series of gated tensor operations which dictate what information to add or remove from the hidden state at each unit.
<br>
<br>
<img src="tensorflow-spam/images/LSTM.png" width=400 height=400 />
<center><sub>This is a graphical representation of a single LSTM unit. The sigma and tanh cells represent sigmoid and hyperbolic tangent functions respectively. X_t and h_t are the input vector and output value of the LSTM unit. </sub></center>
"""

def get_bidirectional_model(tokenizer, lstm_units):
    """
    Constructs the model,
    Embedding vectors => Bi-LSTM => 1 output Fully-Connected neuron with sigmoid activation
    """
    # get the GloVe embedding vectors
    embedding_matrix = get_embedding_vectors(tokenizer)
    # we're going to define the model sequentially https://machinelearningmastery.com/keras-functional-api-deep-learning
    model = Sequential()
    # First, embed the words using loaded GloVe
    model.add(Embedding(len(tokenizer.word_index)+1,
              EMBEDDING_SIZE,
              weights=[embedding_matrix],
              trainable=True,
              mask_zero=True,
              input_length=SEQUENCE_LENGTH))
    # Add bidirectional long short-term memory units
    model.add(Bidirectional(LSTM(lstm_units, recurrent_dropout=0.2)))
    # Add dropout to combat overfitting
    model.add(Dropout(0.2))
    # Add output dense layer with sigmoid for 1/0 classification
    model.add(Dense(1, activation="sigmoid"))
    # compile as rmsprop optimizer
    # aswell as with recall metric
    model.compile(optimizer="rmsprop",
                  loss="binary_crossentropy",
                  metrics=METRICS) # Instead of metrics=[METRICS]

    model.summary()
    return model

"""Call the `get_model` function to generate the skeleton - not trained yet."""

# constructs the bidirectional-model with 128 LSTM units
bimodel = get_bidirectional_model(tokenizer=tokenizer, lstm_units=128)

"""Initialise required TensorBoard callbacks, best model_checkpoints and early stopping. Then fit the model using a batch size and number of epochs defined at the start of section 3.

[Intersting link on explaining feature importance, although I don't know how it will work with the word embedder](https://github.com/slundberg/shap#deep-learning-example-with-deepexplainer-tensorflowkeras-models)

## BiLSTM Model Training
"""



# Commented out IPython magic to ensure Python compatibility.
# initialize our ModelCheckpoint and TensorBoard callbacks
# model checkpoint for saving best weights
model_checkpoint = ModelCheckpoint("../results/spam_classifier_{val_loss:.2f}.h5", save_best_only=True,
                                    verbose=1)
# for better visualization
tensorboard = TensorBoard(log_dir=f"logs/spam_classifier_{time.time()}", histogram_freq=1)
# %tensorboard --logdir ./logs

# define model stopping criteria
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,
                                              restore_best_weights=True)
# train the model
# the metrics parameter has been removed from here and is defined in model.compile
history = bimodel.fit(X_train, y_train, validation_data=(X_test, y_test),
          batch_size=BATCH_SIZE, #64
          epochs=EPOCHS, #20
          callbacks=[tensorboard, model_checkpoint, early_stop],
          verbose=1)
# %tensorboard --logdir logs/fit

# My tensorboard source https://www.youtube.com/watch?v=-9-Hy5dWKLE

"""# Model Scoring"""

# Plot classification model performance metrics

fig, ax = plt.subplots(nrows=2,ncols=2, figsize=(12,8))
ax[0,0].plot(history.history['loss'], label='Train')
ax[0,0].plot(history.history['val_loss'], label='Test')
ax[0,0].set_title('Binomial Cross-Entropy Loss')
ax[0,0].set_ylabel('Loss')
ax[0,0].set_ylim(0,0.3)
ax[0,0].set_xlabel('Epoch')
ax[0,0].legend(loc='upper left')
ax[0,1].plot(history.history['accuracy'], label='Train')
ax[0,1].plot(history.history['val_accuracy'], label='Test')
ax[0,1].set_title('Model Accuracy')
ax[0,1].set_ylabel('Accuracy')
ax[0,1].set_ylim(0.9,1)
ax[0,1].set_xlabel('Epoch')
ax[0,1].legend(loc='lower left')
ax[1,0].plot(history.history['recall'], label='Train')
ax[1,0].plot(history.history['val_recall'], label='Test')
ax[1,0].set_title('Model recall of spam')
ax[1,0].set_ylabel('Recall')
ax[1,0].set_ylim(0.5,1)
ax[1,0].set_xlabel('Epoch')
ax[1,0].legend(loc='lower left')
ax[1,1].plot(history.history['precision'], label='Train')
ax[1,1].plot(history.history['val_precision'], label='Test')
ax[1,1].set_title('Model precision of spam')
ax[1,1].set_ylabel('Precision')
ax[1,1].set_ylim(0.8,1)
ax[1,1].set_xlabel('Epoch')
ax[1,1].legend(loc='lower left')
plt.tight_layout()
plt.savefig('tensorflow-spam/images/progression.png',bbox_inches='tight',dpi=400, pad_inches=0.1)
plt.show()

# Confusion matrix
y_pred = (bimodel.predict(X_test) > 0.5).astype("int32")

plot_confusion_matrix(confusion_matrix(y_test, y_pred), class_names=['Ham','Spam'])
plt.title('Test data confusion matrix with classification threshold at 0.5')
plt.savefig('tensorflow-spam/images/confusion50.png',bbox_inches='tight',dpi=400, pad_inches=0.1)
plt.show()

# get the loss and metrics
result = bimodel.evaluate(X_test, y_test)
# extract those
loss = result[0]
accuracy = result[5]
precision = result[6]
recall = result[7]
f1 = (2*precision*recall)/(precision+recall)

print(f"Accuracy:    {accuracy*100:.2f}%")
print(f"Precision:   {precision*100:.2f}%")
print(f"Recall:      {recall*100:.2f}%")
print(f"F1 Score:    {f1*100:.2f}%")

# Sklearn metrics
probs = list(bimodel.predict(X_test))
outcome = [0 if x[0]<0.5 else 1 for x in list(probs)]
dfpred = pd.DataFrame({'true': y_test,'pred':outcome})

print(confusion_matrix(dfpred.true, dfpred.pred))
print(f'Accuracy   {accuracy_score(dfpred.true, dfpred.pred)*100:.2f}%')
print(f'Precision  {precision_score(dfpred.true, dfpred.pred)*100:.2f}%')
print(f'Recall     {recall_score(dfpred.true, dfpred.pred)*100:.2f}%')
print(f'F1-score   {f1_score(dfpred.true, dfpred.pred)*100:.2f}%')

def get_predictions(text):
    sequence = tokenizer.texts_to_sequences([text])
    # pad the sequence
    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)
    # get the prediction
    prediction = bimodel.predict(sequence)
    if prediction >0.5:
        return 'Spam'
    else:
        return 'Ham'

# Spam test
text = "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
print(get_predictions(text))

# Ham test
text = "Hi man, I was wondering if we can meet tomorrow."
print(get_predictions(text))

# Distribution plots of spam probabilities from bimodel

probs_spam = list(bimodel.predict(X_test))
probs_spam

Y_pp = pd.DataFrame(probs_spam, columns=['prob_spam'])
Y_pp['prob_ham']=Y_pp.prob_spam.apply(lambda x: 1-x)


fig, ax = plt.subplots(ncols=2, figsize=(14,6))
sns.histplot(data=Y_pp,
             x="prob_spam",
             palette='husl',
             bins=75,
             ax=ax[1],
             legend=True).set_title('Distribution of Spam/Ham predicted probabilities with y-axis limited')
sns.histplot(data=Y_pp,
             x="prob_spam",
             palette='husl',
             bins=75,
             ax=ax[0],
             legend=True).set_title('Distribution of Spam/Ham predicted probabilities')
ax[1].set_ylim(0,10)
ax[0].set_xlabel('Probability of Spam')
ax[1].set_xlabel('Probability of Spam')
plt.savefig('tensorflow-spam/images/prob_dist.png',bbox_inches='tight',dpi=400, pad_inches=0.1)

plt.show()

Y_pp

# Define some functions to help, then plot different scores at different thresholds
def predict_at_threshold(x, threshold=0.5):
    if x >= threshold:
        return 1
    else:
        return 0

def scores_at_thresholds(probs, test_data):
    predictions = [probs.apply(predict_at_threshold, threshold=i) for i in np.linspace(0, 1, 100)]
    scores = np.array(
    [(threshold,
      accuracy_score(test_data, predictions[i]),
      precision_score(test_data, predictions[i],zero_division=1),
      recall_score(test_data, predictions[i]),
      f1_score(test_data, predictions[i]),
      confusion_matrix(test_data,predictions[i], normalize='all')[1,0],
      confusion_matrix(test_data,predictions[i], normalize='all')[0,1],
      confusion_matrix(test_data,predictions[i], normalize='all')[0,0],
      confusion_matrix(test_data,predictions[i], normalize='all')[1,1])

     for i, threshold in enumerate(np.linspace(0, 1, 100))])
    return scores


scores = scores_at_thresholds(Y_pp.prob_spam, y_test);

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16,7))

# Plot data
ax1.plot(scores[:, 0], scores[:, 1], label='accuracy')
ax1.plot(scores[:, 0], scores[:, 2], label='precision')
ax1.plot(scores[:, 0], scores[:, 3], label='recall')
ax1.plot(scores[:, 0], scores[:, 4], label='f1')
ax2.plot(scores[:, 0], scores[:, 5], label='FN')
ax2.plot(scores[:, 0], scores[:, 6], label='FP')
ax2.plot(scores[:, 0], scores[:, 7], label='TP')
ax2.plot(scores[:, 0], scores[:, 8], label='TN')

# Adjust legends, titles and labels
ax1.legend(loc=[0.8, 0.25])
ax1.set_xlabel('Threshold')
ax1.set_title('Threshold vs Accuracy, Precision, Recall and F1 for Spam Classification')
ax1.set_ylim(0.8,1)

ax2.legend(loc=[0.8, 0.25])
ax2.set_xlabel('Threshold')
ax2.set_title('Threshold vs Normalised FN, FP, TN and TP for Spam Classification')
ax2.set_ylim(0.0,0.2)

# Annotations
circle_rad = 15  # This is the radius, in points
point=[0.5,0.9885]
ax1.plot(point[0], point[1], 'o', ms=circle_rad * 1, mec='black', mfc='none', mew=1)
ax1.annotate('99% Accuracy at Threshold 0.77', xy=point, xytext=(-120, -55),
            textcoords='offset points',
            color='black', size='large',
            arrowprops=dict(
                arrowstyle='simple,tail_width=0.1,head_width=0.5,head_length=0.5',
                facecolor='black', shrinkB=circle_rad * 1))

point=[0.5,0.95]
ax1.plot(point[0], point[1], 'o', ms=circle_rad * 1, mec='black', mfc='none', mew=1)
ax1.annotate('95% F1 score at Threshold 0.5', xy=point, xytext=(-120, -70),
            textcoords='offset points',
            color='black', size='large',
            arrowprops=dict(
                arrowstyle='simple,tail_width=0.1,head_width=0.5,head_length=0.5',
                facecolor='black', shrinkB=circle_rad * 1))

# point=[0.78,0.025]
# ax2.plot(point[0], point[1], 'o', ms=circle_rad * 1, mec='black', mfc='none', mew=1)
# ax2.annotate('FP=2.5% at Threshold 0.78', xy=point, xytext=(-90, 60),
#             textcoords='offset points',
#             color='black', size='large',
#             arrowprops=dict(
#                 arrowstyle='simple,tail_width=0.1,head_width=0.5,head_length=0.5',
#                 facecolor='black', shrinkB=circle_rad * 1))
plt.savefig('tensorflow-spam/images/threshold.png',bbox_inches='tight',dpi=400, pad_inches=0.1)
plt.show();

Y_pp['pred_class_thresh'] = Y_pp.prob_spam.apply(predict_at_threshold,
                                                threshold=0.5)
print(classification_report(y_test, Y_pp.pred_class_thresh))

Y_pp['pred_class_thresh'] = Y_pp.prob_spam.apply(predict_at_threshold,
                                                threshold=0.77)
print(classification_report(y_test, Y_pp.pred_class_thresh))

# Confusion matrix
y_pred = (bimodel.predict(X_test) > 0.5).astype("int32")

plot_confusion_matrix(confusion_matrix(y_test, y_pred), class_names=['Ham','Spam'])
plt.title('Test data confusion matrix with classification threshold at 0.77')
plt.savefig('tensorflow-spam/images/confusion77.png',bbox_inches='tight',dpi=400, pad_inches=0.1)
plt.show()

# Sklearn metrics
probs = list(bimodel.predict(X_test))
outcome = [0 if x[0]<0.77 else 1 for x in list(probs)]
dfpred = pd.DataFrame({'true': y_test,'pred':outcome})

print(confusion_matrix(dfpred.true, dfpred.pred))
print(f'Accuracy   {accuracy_score(dfpred.true, dfpred.pred)*100:.2f}%')
print(f'Precision  {precision_score(dfpred.true, dfpred.pred)*100:.2f}%')
print(f'Recall     {recall_score(dfpred.true, dfpred.pred)*100:.2f}%')
print(f'F1-score   {f1_score(dfpred.true, dfpred.pred)*100:.2f}%')

bimodel.save('my_model.keras')

# prompt: load model from saved file and use it

import tensorflow as tf

# Load the model
loaded_model = tf.keras.models.load_model('my_model.keras')

# Example usage (assuming you have preprocessed text data in 'text')
def get_predictions(text):
    sequence = tokenizer.texts_to_sequences([text])
    # pad the sequence
    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)
    # get the prediction
    prediction = loaded_model.predict(sequence)
    if prediction >0.5:
        return 'Spam'
    else:
        return 'Ham'

# Example usage:
text = "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's "
print(get_predictions(text))

text = "Hi man, I was wondering if we can meet tomorrow."
print(get_predictions(text))